Go concurrency primitives and scalability
Benchmarking and practical advice
15 Mar 2015

Konstantin Shaposhnikov
k.shaposhnikov@gmail.com


####################################
## Intro

* Why do we write concurrent programs?

.image files/gophers.jpg

# Some well known languages and frameworks are either single threaded (node.js,
# Emacs/elisp, OCaml) or limit scalability to make writing concurrent programs
# easier (Python).


* Concurrency, parallelism and scalability

Talk by Rob Pike:

.link golang.org/s/concurrency-is-not-parallelism

Concurrency is not parallelism, although it enables parallelism.

If you have only one processor, your program can still be concurrent but it
cannot be parallel.

On the other hand, a well-written concurrent program might run efficiently in
parallel on a multiprocessor.

We call program that shows better performance on multiple CPUs scalable.

# In this talk we will look into concurrency primitives in Go and how to use
# them to write scalable programs.


* A word of warning

Prefer clean code to clever code

# You might make a function 5 times faster but if it takes only 1% of total time
# it is not worth it

Always benchmark your program (if performance is important)

# Define import measurements and track them to identify improvements and
# regressions.

Do not rely too much on micro-benchmarks

# I have to use them in this talk to demonstrate some principles. But the only
# real performance measurement is how well the final program performs under
# realistic load conditions on realistic hardware.


######################################
## Example 1

* Example 1: Problem definition

Develop a program that would import temperature data from a CSV file into a
database table (PostgreSQL).

The CSV file has the following structure:

.code files/weather-example.csv

Database table schema:

    create table temperature (
        time varchar(30),
        city varchar(30),
        value varchar(30)
    )


* Solution 1: Lets go concurrent!

*PostgreSQL* is a highly scalable database!

.code -numbers csv2db-concurrent/main.go /START_MAIN OMIT/,/END_MAIN OMIT/


* Solution 1: Lets go concurrent! (continued...)

.code -numbers csv2db-concurrent/main.go /START_HL OMIT/,/END_HL OMIT/


* Solution 1: Lets run it

On a file with 1 million rows:

    wc -l t.csv
    head t.csv

Using different GOMAXPROCS values

    for i in 1 2 4 8 16 32 40 64 ; do
        echo -n "$i " >&2
        GOMAXPROCS=$i /usr/bin/time -f "%e" csv2db-concurrent < t.csv > /dev/null
    done

Check results:

    echo "select count(*) from temperature" | sudo -u postgres psql d


* Solution 1: Conclusion

.image files/csv2db-concurrent.png

Observations:

- Is it fast? 4 minutes. Not impressive.
- Does it scale? Nope.


* Solution 2: Single threaded batching

.code -numbers csv2db-batch/main.go /START_MAIN OMIT/,/END_MAIN OMIT/


* Solution 2: Lets run it

Run:

    time csv2db-batch < t.csv

And check results:

    echo "select count(*) from temperature" | sudo -u postgres psql d


* Solution 2: Conclusion

2 seconds!


#########################
## Example 2: Pi

* Example 2: Pi

Develop a program to calculate value of Pi using Monte Carlo method.

.image files/Pi-MonteCarlo.png


* Monte Carlo

.code -numbers pi/main.go /START_MC OMIT/,/END_MC OMIT/


* Lets call it concurrently!

Generate 100 million points using NumCPU() goroutines

.code -numbers pi/main.go /START_MAIN OMIT/,/END_MAIN OMIT/


* Lets run it

    for i in 1 2 4 8 16 32 40 64 ; do
        echo -n "$i " >&2
        GOMAXPROCS=$i /usr/bin/time -f "%e" pi > /dev/null
    done


* First results

.image files/pi.png


* What a surprise

.image files/concurrency.jpg


* Parallel benchmarks

From *https://golang.org/pkg/testing*:

If a benchmark needs to test performance in a parallel setting, it may use the
RunParallel helper function:

    func BenchmarkTemplateParallel(b *testing.B) {
        templ := template.Must(template.New("test").Parse("Hello, {{.}}!"))
        b.RunParallel(func(pb *testing.PB) {
            var buf bytes.Buffer
            for pb.Next() {
                buf.Reset()
                templ.Execute(&buf, "World")
            }
        })
    }

Such benchmarks are intended to be used with the go test -cpu flag:

    go test -bench BenchmarkTemplateParallel -cpu 1,2,4,8,16,32,40,64


* Lets benchmark rand.Float64()

.code benchmarks/rand_test.go /START_GLOBAL OMIT/,/END_GLOBAL OMIT/

And run it:

  go test talk/benchmarks -bench BenchmarkRandFloat64_Global -cpu 1,2,4,8,16,32,40,64


* rand.Float64()

    BenchmarkRandFloat64_Global             50000000          32.6 ns/op
    BenchmarkRandFloat64_Global-2           20000000          69.0 ns/op
    BenchmarkRandFloat64_Global-4           10000000           234 ns/op
    BenchmarkRandFloat64_Global-8            5000000           421 ns/op
    BenchmarkRandFloat64_Global-16           5000000           336 ns/op
    BenchmarkRandFloat64_Global-32           5000000           320 ns/op
    BenchmarkRandFloat64_Global-40           5000000           298 ns/op
    BenchmarkRandFloat64_Global-64           5000000           256 ns/op


rand.Float64() doesn't scale

Q: Why?
Q: It uses sync.Mutex internally

From *https://golang.org/pkg/math/rand*: The default Source is safe for
concurrent use by multiple goroutines.


* Attempt 1: lets use channel

.code benchmarks/rand_test.go /START_CHANNEL OMIT/,/END_CHANNEL OMIT/

And run it:

  go test talk/benchmarks -bench BenchmarkRandFloat64_Channel -cpu 1,2,4,8,16,32,40,64

* Still not good

    BenchmarkRandFloat64_Channel     20000000          73.7 ns/op
    BenchmarkRandFloat64_Channel-2   10000000          137 ns/op
    BenchmarkRandFloat64_Channel-4   10000000          167 ns/op
    BenchmarkRandFloat64_Channel-8   5000000           228 ns/op
    BenchmarkRandFloat64_Channel-16  5000000           311 ns/op
    BenchmarkRandFloat64_Channel-32  5000000           335 ns/op
    BenchmarkRandFloat64_Channel-40  3000000           396 ns/op
    BenchmarkRandFloat64_Channel-64  3000000           434 ns/op

Actually the results look even worse than using a global Mutex.

The operation to generate a random number is relatevely inexpensive so channel
overhead becomes significant in this case.


* Lets use an idea from other languages

Java has java.util.concurrent.ThreadLocalRandom:

    A random number generator isolated to the current thread. ...  When
    applicable, use of ThreadLocalRandom rather than shared Random objects in
    concurrent programs will typically encounter much less overhead and
    contention.

But there is no thread local storage in Go.

Can we use something else?


* Attempt 2: sync.Pool

.code benchmarks/rand_test.go /START_POOL OMIT/,/END_POOL OMIT/

And run it:

  go test talk/benchmarks -bench BenchmarkRandFloat64_Pool -cpu 1,2,4,8,16,32,40,64


* Success?

    BenchmarkRandFloat64_Pool       30000000            41.6 ns/op
    BenchmarkRandFloat64_Pool-2     50000000            21.1 ns/op
    BenchmarkRandFloat64_Pool-4     100000000           11.0 ns/op
    BenchmarkRandFloat64_Pool-8     200000000           6.01 ns/op
    BenchmarkRandFloat64_Pool-16	300000000           3.45 ns/op
    BenchmarkRandFloat64_Pool-32	100000000           10.9 ns/op
    BenchmarkRandFloat64_Pool-40	200000000           6.56 ns/op
    BenchmarkRandFloat64_Pool-64	500000000           4.20 ns/op


It scales though a bit unreliably.

The implementation of sync.Pool is optimized to scale.

Q: Can we do better?


* Attempt 3: do not share state

.code benchmarks/rand_test.go /START_SOURCE OMIT/,/END_SOURCE OMIT/

And run it:

  go test talk/benchmarks -bench BenchmarkRandFloat64_Source -cpu 1,2,4,8,16,32,40,64


* Success

    BenchmarkRandFloat64_Source         100000000            13.6 ns/op
    BenchmarkRandFloat64_Source-2       200000000            6.76 ns/op
    BenchmarkRandFloat64_Source-4       500000000            3.50 ns/op
    BenchmarkRandFloat64_Source-8       2000000000           1.75 ns/op
    BenchmarkRandFloat64_Source-16      2000000000           1.13 ns/op
    BenchmarkRandFloat64_Source-32      2000000000           0.82 ns/op
    BenchmarkRandFloat64_Source-40      2000000000           0.67 ns/op
    BenchmarkRandFloat64_Source-64      2000000000           0.80 ns/op


This is so fast and scalable.

Lesson: do not share state!


* Back to Pi: Pi Improved

.code pii/main.go /START_MC OMIT/,/END_MC OMIT/

Run it

    for i in 1 2 4 8 16 32 40 64 ; do
        echo -n "$i " >&2
        GOMAXPROCS=$i /usr/bin/time -f "%e" pii > /dev/null
    done


* Final Results

.image files/pii.png


##########################
## Example 3

* Example 3: Maps

Lets implement a data structure to store application configuration that is often
read from multiple goroutines but doesn't change or changes very infrequently.

We will use *map[string]string* as storage data structure.


* Maps are not safe for concurrent use.

Really?

Concurrent reads are safe as long as all writes "happen before" (as per
*https://golang.org/ref/mem*) these reads.

    m := readConfig()
    go func() {
        ...
       readConfig(m)
       ...
    }
    go func() {
        ...
       readConfig(m)
       ...
    }


* Read only map can be accessed from multiple threads

.code -numbers benchmarks/map_test.go /START_RO/,/END_RO/


* Read only results

Race detector doesn't complain:

    go test talk/benchmarks -bench BenchmarkMap_Readonly -race

As expected it is also fast and scales:

    go test talk/benchmarks -bench BenchmarkMap_Readonly -cpu 1,2,4,8,16,32,40,64

    BenchmarkMap_Readonly-2     10000000            160 ns/op
    BenchmarkMap_Readonly-4     20000000            84.8 ns/op
    BenchmarkMap_Readonly-8     30000000            40.0 ns/op
    BenchmarkMap_Readonly-16	50000000            32.5 ns/op
    BenchmarkMap_Readonly-32	50000000            29.5 ns/op
    BenchmarkMap_Readonly-40	50000000            30.7 ns/op
    BenchmarkMap_Readonly-64	30000000            35.1 ns/op


* Writes require synchronization

.code -numbers benchmarks/map_test.go /START_NOSYNC/,/END_NOSYNC/


* ... continued

As expected race detector quickly finds a data race:

    go test talk/benchmarks -bench BenchmarkMap_NoSync -race

But even without race detector enabled it fails (Go 1.6+):

    go test talk/benchmarks -bench BenchmarkMap_NoSync


* sync.Mutex

.code -numbers benchmarks/map_test.go /START_MUTEX/,/END_MUTEX/

* sync.Mutex: results

    go test talk/benchmarks -bench BenchmarkMap_Mutex -cpu 1,2,4,8,16,32,40,64


As expected it doesn't scale:

    BenchmarkMap_Mutex       5000000           325 ns/op
    BenchmarkMap_Mutex-2     3000000           558 ns/op
    BenchmarkMap_Mutex-4     2000000           655 ns/op
    BenchmarkMap_Mutex-8     2000000           632 ns/op
    BenchmarkMap_Mutex-16    3000000           569 ns/op
    BenchmarkMap_Mutex-32    2000000           554 ns/op
    BenchmarkMap_Mutex-40    3000000           580 ns/op
    BenchmarkMap_Mutex-64    3000000           586 ns/op


* sync.RWMutex

.code -numbers benchmarks/map_test.go /START_RWMUTEX/,/END_RWMUTEX/

* sync.RWMutex: results

    go test talk/benchmarks -bench BenchmarkMap_RWMutex -cpu 1,2,4,8,16,32,40,64

Scales but not so good:

    BenchmarkMap_RWMutex       5000000           328 ns/op
    BenchmarkMap_RWMutex-2    10000000           208 ns/op
    BenchmarkMap_RWMutex-4    10000000           141 ns/op
    BenchmarkMap_RWMutex-8    20000000           164 ns/op
    BenchmarkMap_RWMutex-16   10000000           130 ns/op
    BenchmarkMap_RWMutex-32   20000000           121 ns/op
    BenchmarkMap_RWMutex-40   20000000           105 ns/op
    BenchmarkMap_RWMutex-64   20000000           107 ns/op


* atomic.Value and copy on write

.code -numbers benchmarks/map_test.go /START_ATOMIC/,/END_ATOMIC/


* atomic.Value results

    go test talk/benchmarks -bench BenchmarkMap_Atomic -cpu 1,2,4,8,16,32,40,64

Almost as good as non synchronized access

    BenchmarkMap_Atomic      5000000            334 ns/op
    BenchmarkMap_Atomic-2   10000000            170 ns/op
    BenchmarkMap_Atomic-4   20000000            86.0 ns/op
    BenchmarkMap_Atomic-8   30000000            48.0 ns/op
    BenchmarkMap_Atomic-16  50000000            34.0 ns/op
    BenchmarkMap_Atomic-32  50000000            30.4 ns/op
    BenchmarkMap_Atomic-40  50000000            33.3 ns/op
    BenchmarkMap_Atomic-64  30000000            39.4 ns/op


* Q&A
